---
title: "p8105_hw6_brm2150"
author: "Brooklynn McNeil"
date: "2024-11-19"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(plotly)
library(modelr)
library(caret)

knitr::opts_chunk$set(
  comment = '', fig.width = 8, fig.height = 6, out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

## Problem 2

Load the homicide data. Create a binary variable for the status for solved/not solved. Filter out cities that don't report race, and only include Black and White races

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_dat = 
  read_csv(url, na = "Unknown") |>
  mutate(city_state = str_c(city, ", ", state),
         status = case_when(
           disposition %in% c("Closed without arrest", "Closed by arrest") ~ 0,
           disposition == "Open/No arrest" ~ 1
           )) |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black"))

head(homicide_dat)
```
Create a logistic regression for `status` as the outcome and `age`, `race`, and `sex` as predictors for Baltimore, MD. 

```{r}
baltimore_glm = 
  homicide_dat |>
  filter(city_state == "Baltimore, MD") |>
  glm(status ~ victim_age + victim_race + victim_sex, data = _, family = "binomial")

tidy_results = broom::tidy(baltimore_glm, conf.int = TRUE, exponentiate = TRUE)

# show odds ratio for men
tidy_results |>
  filter(term == "victim_sexMale") |>
  select(-c(std.error, statistic, p.value))
```

Now let's map this across all of the cities, and then plot the odds ratios for murders of men being solved compared to compare across cities.

```{r}
citywide_odds = 
  homicide_dat |>
  group_by(city_state) |>
  nest() |>
  mutate(
    glm = map(data, \(df) glm(status ~ victim_age + victim_race + victim_sex, data = df, family = "binomial")),
    glm = map(glm, \(i) broom::tidy(i, conf.int = TRUE, exponentiate = TRUE)),
  ) |>
  unnest(glm) |>
  filter(term == "victim_sexMale")

citywide_odds |>
  mutate(city_state = factor(city_state, levels = citywide_odds$city_state[order(citywide_odds$estimate)])) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Odds Ratios for Homicides Status by Sex",
       x = ("City, State"),
       y = "odds ratio")
```
It looks like most cities have a higher ratio of homicides of men getting solved than to women. The city where the highest ratio is in New York. Long Beach, CA has a very wide confidence interval, but even so the interquantile range does not include an odds ratio that men's homcides are not solved more often. 

## Problem 3

Read in the birth weight data. 

```{r}
birthweight_df = 
  read_csv("data/birthweight.csv") |>
  mutate(across(c("babysex", "frace", "malform", "mrace"), factor))

any(is.na(birthweight_df))
```

Let's take a quick look at the data. It's normally distributed. 
```{r}
birthweight_df |>
  ggplot(aes(x = bwt)) +
  geom_histogram()
```

Let's fit a generalized model with all predictors using `glm` and then narrow it down with `step_AIC`. Make sure variance inflation factor is <5 to check for colinearity of predictors. 

```{r}
fit = 
  birthweight_df |>
  glm(bwt ~ gaweeks + babysex + bhead + blength + delwt + fincome + frace + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = _ )

summary(fit)

# remove unnecessary predictors
best_fit = MASS::stepAIC(fit, trace = FALSE)

summary(best_fit)
plot(best_fit)

# check for multicollinearity
car::vif(best_fit)
```

Plot the modeled residuals against fitted values.

```{r}
birthweight_df |>
  add_predictions(best_fit) |>
  add_residuals(best_fit) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Estimates vs. Residuals for Best Fit",
       x = "predictions",
       y = "residuals")
```

Compare the `best_fit` model with two other models. One using length at birth and gestational age as predictors (main effects only) and one using head circumference, length, sex, and all interactions (including the three-way interaction) between these.

```{r}
mod_1 = birthweight_df |>
  glm(bwt ~ gaweeks + blength, data = _ )

mod_2 = birthweight_df |>
  glm(bwt ~ babysex + bhead + blength + babysex * bhead * blength, data = _ )
```

Compare the model performance by creating 1000 test and train data sets, then calculating rmse for each set. It looks like the `best_fit` model outperforms the others.
```{r}
set.seed(123)
results = 
  tibble(
    crossv_mc(birthweight_df, n = 1000),
    best_fit = map(train, \(df) glm(bwt ~ gaweeks + babysex + bhead + blength + delwt + 
    fincome + mheight + mrace + parity + ppwt + smoken, data = df)),
    mod_1 = map(train, \(df) glm(bwt ~ gaweeks + blength, data = df )),
    mod_2 = map(train, \(df) glm(bwt ~ babysex + bhead + blength + babysex * bhead * blength, data = df )),
    best_fit_rmse = map2_dbl(best_fit, test, \(x, y) rmse(x, y)),
    mod_1_rmse = map2_dbl(mod_1, test, \(x, y) rmse(x, y)),
    mod_2_rmse = map2_dbl(mod_2, test, \(x, y) rmse(x, y))
  )

results |>
  pivot_longer(cols = best_fit_rmse:mod_2_rmse,
               names_to = "model",
               values_to = "rmse") |>
  mutate(model = gsub("_rmse", "", model)) |>
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin() +
  labs(title = "RMSE for Model Comparison")
```

